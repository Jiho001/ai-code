gpt-3같은 초대형 언어모델들은, 사용할 수 있는 컴퓨팅 자원이 정해져있기 때문에 pretrain 단계에서 한번만 학습이 가능했다. 따라서 최적의 하이퍼 파라미터를 추정하는 것은 매우 중요했다. 

Kaplan의 주장: 컴퓨팅 최적 학습을 위해서는 주어진 컴퓨팅 버젯 하에서 모델을 더 오래 학습시켜 손실을 최대한 낮추는 것보다, 차라리 모델을 더 크게 만들고 조금만 학습하는 것이 전체 손실 감소에 더 효율적이다. 

해당 주장에 따라 NLP 분야는 모델 크기를 키우는 것에 더 집중하기 시작했다. 

본 친칠라 논문의 저자도 동일한 결론을 내렸지만, 대규모 모델은 Kaplan이 추천한 것보다 더많은 데이터로 학습되어야한다고 추정했다.

[Kaplan의 주장]
컴퓨팅 예산 10배 증가 - 모델 크기 5.5 배 증가 - 학습 데이터 1.8배 증가
[친칠라 주장]
모델 크기와 학습 데이터 수는 동일한 비율로 스케일링 되어야한다.

Kaplan과 GPT-3(2020)의 학습 방식을 따라서 최근 대형 언어모델들은 약 300B(3000억) 토큰 정도의 데이터로 학습되었다.
계산 자원을 늘릴 때 학습 크기를 주로 늘리는 방식에 따른 것이다.

FLOP(Floating Poing Operation)
부동소수점 연산 1회를 의미하는 단위. 즉 FLOPs는 모델 학습에 필요한 총 연산 횟수
=> 학습 컴퓨팅 비용으로 생각

주어진 FLOPs 예산 하에서, 모델 크기와 학습 데이터 양을 어떻게 조정해야 가장 효율적인가? 에 대한 질문을 다시 다룬다.
이를 위해서 최종 pretraining 로스 L(N, D)를 모델 파라미터수 N과 학습 토큰 수 D의 함수로 표현한다.
필요한 컴퓨팅 비용은 파라미터수 N과 학습 토큰수 D가 주어지면 결정되는 값이기 때문에, C = FLOPs(N, D)로 표현할 수 있다.
즉 우리는 C = FLOPs(N, D)라는 가정하에 손실 L을 최소화하는 N과 D의 조합을 찾고자한다.
N(C), D(C)는 주어진 계산 예산 C에 대한 최적의 모델 크기와 데이터 크기를 나타낸다.
저자들은 이 함수에 대한 경험적 추정을 위해, 파라미터 수: 7천만~160억인 400개 이상의 모델을 학습 토큰 수 50억(5B)~4000억(400B) 범위 내에서 직접 학습시켜봤다.
각 모델은 여러 학습 기간(학습 스텝)으로 훈련되었다. 
결과적으로 Kaplan과 완전히 다른 스케일링 법칙이 도출되었다.

저자들의 계산 결과에 따르면, Gopher를 학습할 때 사용한 동일 컴퓨팅 예산에서
최적 모델은 고퍼보다 4배 더 작고, 4배 더 많은 토큰으로 학습되어야한다고 예측된다.
이를 검증하기 위해 700억(70B) 파라미터 모델인 "친칠라"를 1.4조(1.4T - 1400B) 토큰으로 학습시켰다.
그 결과, 2800억(280B) 파라미터 모델인 고퍼를 성능면에서 완전히 능가했다.
또한 모델 크기가 줄어들면서 추론 비용이 크게 감소하고, 작은 하드웨어에서의 활용도 훨씬 쉬워졌다.  
* 고퍼의 학습 토큰 수는 300B이다.

### 2 Related Work
Large language models: 가장 큰 dense transformer 모델은 5천억개 이상의 파라미터 - 더 크고 큰 모델을 훈련하려는 경향. 그리고 지금까지의 크기를 늘리려는 시도는 SOTA를 갱신하는데 기여했음.
그럼에도 불구하고, 이런 방향은 압도적인 계산 요구 사항과 더 많은 고품질 훈련 데이터 획득의 필요성을 포함하여 여러가지 문제에 직면해있음.

Scaling behavior 모델링: Kaplan도 주어진 연산량안에서 모델 크기 N과 데이터수D를 어떻게 조합해야 최적인가? 를 찾으려했음. 하지만 데이터양(D)를 고정시킨채로, 모델 크기만 변화시키면서 찾는 방식을 사용.
또한 kaplan은 모든 모델이 130B 토큰을 기준으로 동일한 learning rate decay schedule을 사용함 - 즉, 학습을 130B 토큰만큼만 진행한다고 가정한 스케쥴로, 중간 손실도 그것에 맞춰 측정 - 그런데 만약 어떤 모델이 중간 손실을 계산할 때 그보다 적은 토큰만 학습됐다면
